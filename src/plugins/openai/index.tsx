/**
 * çœ‹çœ‹ç¾¤å‹ä»¬éƒ½èŠäº†ä»€ä¹ˆå‹¾å…«.jpg
 * @author dragon-fish
 * @license MIT
 */
import { Context, Session, Time, arrayBufferToBase64 } from 'koishi'

import crypto from 'node:crypto'
import { readFileSync, writeFileSync } from 'node:fs'
import { readFile } from 'node:fs/promises'
import { resolve } from 'node:path'

import BasePlugin from '~/_boilerplate'

import { getUserNickFromSession } from '$utils/formatSession'
import { safelyStringify } from '$utils/safelyStringify'
import { ClientOptions, OpenAI } from 'openai'
import { CompletionUsage } from 'openai/resources/completions'

declare module 'koishi' {
  export interface Tables {
    openai_chat: OpenAIConversationLog
  }
  export interface User {
    openai_last_conversation_id: string
  }
}

interface OpenAIConversationLog {
  id: number
  conversation_id: string
  conversation_owner: number
  role: 'system' | 'user' | 'assistant'
  content: string
  usage?: CompletionUsage
  model?: string
  time: number
}

export interface Config {
  openaiOptions: ClientOptions
  model: string
  maxTokens: number
  recordsPerChannel: number
  modelAliases?: Record<string, string>
}

export default class PluginOpenAi extends BasePlugin {
  static inject = ['html', 'database']

  openai: OpenAI
  openaiOptions: ClientOptions
  SILI_PROMPT = PluginOpenAi.readPromptFile('SILI-v3.md')
  CHAT_SUMMARY_PROMPT = PluginOpenAi.readPromptFile('chat-summary.txt')
  CENSOR_PROMPT = PluginOpenAi.readPromptFile('censor.txt')
  RANDOM_ERROR_MSG = (
    <random>
      <template>SILIä¸çŸ¥é“å–”ã€‚</template>
      <template>è¿™é“é¢˜SILIä¸ä¼šï¼Œé•¿å¤§ååœ¨å­¦ä¹ ~</template>
      <template>SILIçš„å¤´å¥½ç—’ï¼Œä¸ä¼šè¦é•¿è„‘å­äº†å§ï¼Ÿï¼</template>
      <template>é”Ÿæ–¤æ‹·é”Ÿæ–¤æ‹·é”Ÿæ–¤æ‹·</template>
    </random>
  )
  #chatRecords: Record<string, Session.Payload[]> = {}
  readonly modelAliases: Record<string, string> = {}

  constructor(
    ctx: Context,
    config: Partial<Config> = { recordsPerChannel: 100 }
  ) {
    super(ctx, config, 'openai')

    this.openaiOptions = config.openaiOptions || {}
    this.openai = new OpenAI({
      ...this.openaiOptions,
    })
    this.#initDatabase()
    this.#handleRecordsLog().then(() => {
      this.#initListeners()
      this.#initCommands()
    })
    if (config.modelAliases) {
      this.modelAliases = config.modelAliases
    }
  }

  async #initDatabase() {
    this.ctx.model.extend('user', {
      openai_last_conversation_id: 'string',
    })
    this.ctx.model.extend(
      'openai_chat',
      {
        id: 'integer',
        conversation_id: 'string',
        conversation_owner: 'integer',
        role: 'string',
        content: 'string',
        usage: 'json',
        model: 'string',
        time: 'integer',
      },
      {
        primary: 'id',
        autoInc: true,
      }
    )
  }
  async #handleRecordsLog() {
    const logFile = resolve(__dirname, 'records.log')
    try {
      const text = (await readFile(logFile)).toString()
      const obj = JSON.parse(text)
      this.#chatRecords = obj
    } catch (_) {}

    process.on('exit', () => {
      try {
        writeFileSync(logFile, safelyStringify(this.#chatRecords))
      } catch (e) {
        console.info('save logs error', e)
      }
    })
  }

  #initListeners() {
    this.ctx.channel().on('message', this.addRecord.bind(this))
    this.ctx.channel().on('send', this.addRecord.bind(this))
  }

  #initCommands() {
    this.ctx.command('openai', 'Make ChatBot Great Again')

    this.ctx
      .channel()
      .command('openai/chat-summary', 'ç¾¤é‡Œåˆšåˆšéƒ½èŠäº†äº›ä»€ä¹ˆ', {
        authority: 2,
      })
      .alias('æ€»ç»“èŠå¤©', 'ç¾¤é‡ŒåˆšåˆšèŠäº†ä»€ä¹ˆ')
      .option('number', '-n <number:posint>', { hidden: true })
      .option('channel', '-c <channel:string>', { hidden: true })
      .action(async ({ session, options }) => {
        await session.send(
          <>
            <quote id={session.messageId}></quote>ç¨ç­‰ï¼Œè®©æˆ‘çœ‹çœ‹èŠå¤©è®°å½•â€¦â€¦
          </>
        )
        const msg = await this.summarize(options.channel || session.channelId)
        return msg
      })

    this.ctx
      .command('openai.models', 'List models', { authority: 3 })
      .action(async () => {
        const { data } = await this.openai.models.list()
        this.logger.info('openai.models', data)
        return (
          <>
            <p>Currently available models:</p>
            <p>{data.map((i) => i.id).join('\n')}</p>
          </>
        )
      })

    this.ctx
      .command('openai/chat <content:text>', 'ChatGPT', {
        minInterval: 1 * Time.minute,
        bypassAuthority: 3,
        maxUsage: 10,
      })
      .shortcut(/(.+)[\?ï¼Ÿ]$/, {
        args: ['$1'],
        prefix: true,
      })
      .alias()
      .option('prompt', '-p <prompt:string>', {
        hidden: true,
        authority: 3,
      })
      .option('model', '-m <model:string>', {
        hidden: true,
        authority: 3,
      })
      .option('debug', '-d', { hidden: true, authority: 3 })
      .userFields(['id', 'name', 'openai_last_conversation_id', 'authority'])
      .check(({ options }) => {
        if (options.model) {
          const maybeRealModel = this.modelAliases[options.model]
          if (maybeRealModel) {
            options.model = maybeRealModel
          }
        }
      })
      .action(async ({ session, options }, content) => {
        this.logger.info('[chat] input', options, content)

        const startTime = Date.now()
        const conversation_owner = session.user.id
        const userName = getUserNickFromSession(session)

        const conversation_id: string =
          (session.user.openai_last_conversation_id ||= crypto.randomUUID())

        const histories = await this.getChatHistoriesById(conversation_id)
        this.logger.info('[chat] user data', {
          conversation_owner,
          conversation_id,
          historiesLenth: histories.length,
        })

        const model = options.model || this.config.model || 'gpt-4o-mini'
        const stream = await this.openai.chat.completions.create(
          {
            model,
            messages: [
              // magic
              // {
              //   role: 'system',
              //   content: `You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2021-09\nCurrent model: ${
              //     options.model || 'gpt-3.5-turbo'
              //   }\nCurrent time: ${new Date().toLocaleString()}`,
              // },
              // base prompt
              {
                role: 'system',
                content: options.prompt || this.SILI_PROMPT,
              },
              // provide user info
              {
                role: 'system',
                content: `The person talking to you: ${userName}\nCurrent time: ${new Date().toLocaleString()}\n`,
              },
              // chat history
              ...histories,
              // current user input
              { role: 'user', content },
            ],
            max_tokens: this.config.maxTokens ?? 1024,
            temperature: 0.9,
            presence_penalty: 0.6,
            frequency_penalty: 0,
            stream: true,
            stream_options: {
              include_usage: true,
            },
          },
          { timeout: 60 * 1000 }
        )

        // è¯»å–æµå¼æ•°æ®
        let fullThinking = ''
        let fullContent = ''
        let sendContentFromIndex = 0
        let sendThinkingFromIndex = 0
        let usage: CompletionUsage | undefined
        let thinkingEnd = false
        const shouldSendThinking = options.debug

        // å¦‚æœæ²¡æœ‰å¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œæ¯æ€è€ƒ 10 ç§’å‘é€ä¸€ä¸ªçŠ¶æ€æŒ‡ç¤ºå™¨
        let sendStatusIndicator = -1
        const indicators = ['181', '285', '267', '312', '284', '37']
        const interval = setInterval(() => {
          if (
            sendContentFromIndex ||
            sendThinkingFromIndex ||
            Date.now() - startTime > 60 * 1000
          ) {
            clearInterval(interval)
          } else {
            sendStatusIndicator = (sendStatusIndicator + 1) % indicators.length
            session.onebot?._request('set_msg_emoji_like', {
              message_id: session.messageId,
              emoji_id: indicators[sendStatusIndicator],
            })
          }
        }, 10 * 1000)

        // #region chat-stream
        for await (const chunk of stream) {
          if (chunk.usage) {
            usage = chunk.usage
          }
          const thinking: string =
            (chunk as any).choices?.[0]?.delta?.reasoning_content?.trim() || ''
          const content = chunk.choices?.[0]?.delta?.content?.trim() || ''

          // å†…å¿ƒç‹¬ç™½
          if (thinking) {
            fullThinking += thinking
            if (shouldSendThinking) {
              const { text, nextIndex } = this.splitContent(
                fullThinking,
                sendThinkingFromIndex
              )
              sendThinkingFromIndex = nextIndex
              text && (await session.sendQueued('[å†…å¿ƒç‹¬ç™½] ' + text))
            }
          }
          // å†…å¿ƒç‹¬ç™½ç»“æŸ
          if (content && !thinkingEnd) {
            thinkingEnd = true
            this.logger.info('[chat] think end:', fullThinking)
            if (
              fullThinking &&
              sendThinkingFromIndex < fullThinking.length &&
              shouldSendThinking
            ) {
              await session.sendQueued(
                '[å†…å¿ƒç‹¬ç™½] ' + fullThinking.slice(sendThinkingFromIndex)
              )
            }
          }
          // æ­£æ–‡å†…å®¹
          if (content) {
            fullContent += content
            const { text, nextIndex } = this.splitContent(
              fullContent,
              sendContentFromIndex
            )
            sendContentFromIndex = nextIndex
            if (text) {
              this.logger.info('[chat] sending:', text)
              await session.sendQueued(text)
            }
          }
        }
        //#endregion

        // å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if (sendContentFromIndex < fullContent.length) {
          const text = fullContent.slice(sendContentFromIndex)
          this.logger.info('[chat] send remaining:', text)
          await session.sendQueued(text)
        }

        this.logger.success('[chat] stream end:', {
          fullThinking,
          fullContent,
          usage,
        })

        if (fullContent) {
          // save conversations to database
          ;[
            { role: 'user', content, time: startTime },
            {
              role: 'assistant',
              content: fullContent,
              time: Date.now(),
              usage,
              model,
            },
          ].forEach((item) =>
            // @ts-ignore
            this.ctx.database.create('openai_chat', {
              ...item,
              conversation_owner,
              conversation_id,
            })
          )
        }
      })

    this.ctx
      .command('openai/chat.reset', 'å¼€å§‹æ–°çš„å¯¹è¯')
      .userFields(['openai_last_conversation_id'])
      .action(async ({ session }) => {
        if (!session.user.openai_last_conversation_id) {
          return (
            <random>
              <>å—¯â€¦â€¦æˆ‘ä»¬å¥½åƒè¿˜æ²¡èŠè¿‡ä»€ä¹ˆå‘€â€¦â€¦</>
              <>å’¦ï¼Ÿä½ è¿˜æ²¡æœ‰å’ŒSILIåˆ†äº«è¿‡ä½ çš„æ•…äº‹å‘¢ï¼</>
              <>æ¬¸ï¼ŸSILIå¥½åƒè¿˜æ²¡å’Œä½ è®¨è®ºè¿‡ä»€ä¹ˆå“¦ã€‚</>
            </random>
          )
        } else {
          session.user.openai_last_conversation_id = ''
          return (
            <random>
              <>è®©æˆ‘ä»¬å¼€å§‹æ–°è¯é¢˜å§ï¼</>
              <>å—¯â€¦â€¦é‚£æˆ‘ä»¬èŠç‚¹åˆ«çš„å§ï¼</>
              <>å¥½å§ï¼Œé‚£æˆ‘å°±ä¸æä¹‹å‰çš„äº‹äº†ã€‚</>
              <>ä½ æœ‰æ›´å¥½çš„ç‚¹å­å’ŒSILIåˆ†äº«å—ï¼Ÿ</>
              <>å’¦ï¼Ÿæ˜¯è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿ</>
            </random>
          )
        }
      })

    this.ctx
      .command(
        'openai.tts <input:text>',
        'Generates audio from the input text',
        {
          maxUsage: 3,
          bypassAuthority: 3,
        }
      )
      .option('model', '-m <model:string> tts-1 or tts-1-hd')
      .option(
        'voice',
        '-v <voice:string> alloy, echo, fable, onyx, nova, and shimmer'
      )
      .option('speed', '-s <speed:number> 0.25 - 4.0')
      .action(async ({ options }, input) => {
        if (!input) {
          return 'SILIä¸çŸ¥é“ä½ æƒ³è¯´ä»€ä¹ˆå‘¢ã€‚'
        }

        options = Object.fromEntries(
          Object.entries(options).filter(([, val]) => !!val)
        )

        const buffer = await this.createTTS(input, options as any)
        const base64 = arrayBufferToBase64(buffer)
        return <audio src={`data:audio/mp3;base64,${base64}`}></audio>
      })
  }

  /**
   * æå‡å¯¹è¯è¿è´¯æ€§ï¼Œå°†å¯¹è¯å†…å®¹åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†
   *
   * é¦–å…ˆæŠ›å¼ƒ fromIndex ä¹‹å‰çš„å†…å®¹ï¼Œå‰©ä¸‹çš„ç§°ä¸º rest
   * å°† rest æŒ‰ç…§ splitChars ä¸­çš„å­—ç¬¦è¿›è¡Œåˆ†å‰²ï¼Œå¾—åˆ°åˆ†å‰²ç‚¹çš„ç´¢å¼•
   * å¦‚æœåˆ†å‰²ç‚¹çš„æ•°é‡å¤§äº expectPartsï¼Œè¿”å›å‰ expectParts ä¸ªåˆ†å‰²ç‚¹ä¹‹é—´çš„å†…å®¹ï¼ŒnextIndex ä¸ºç¬¬ expectParts ä¸ªåˆ†å‰²ç‚¹çš„ç´¢å¼•ï¼ˆæ³¨æ„æ˜¯åŸºäº fullText çš„ç´¢å¼•ï¼‰
   * å¦‚æœåˆ†å‰²ç‚¹çš„æ•°é‡å°äº expectPartsï¼Œä»€ä¹ˆä¹Ÿä¸åšï¼šè¿”å› text ä¸ºç©ºå­—ç¬¦ä¸²ï¼ŒnextIndex ä¸º fromIndex
   * å¦‚æœå‰©ä½™çš„å†…å®¹é•¿åº¦å¤§äº maxLengthï¼Œå°è¯•å‡å°‘ expectParts çš„æ•°é‡ï¼Œç›´åˆ°å‰©ä½™é•¿åº¦å°äº maxLength
   * å¦‚æœ expectParts ä¸º 0ï¼Œæ„å‘³ç€å‰©ä½™çš„å†…å®¹æ²¡æœ‰åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œä½œä¸ºä¿åº•æœºåˆ¶ï¼ŒæŠŠå‰©ä½™å†…å®¹ç›´æ¥è¿”å›ï¼ŒnextIndex è®¾ç½®åˆ°æœ«å°¾
   * æ³¨æ„ï¼šè¿”å›çš„ nextIndex éƒ½æ˜¯åŸºäº fullText çš„ç´¢å¼•ï¼Œå¦‚æœåŸºäº rest è®¡ç®—è¦åŠ ä¸Š fromIndex
   *
   * @param fullText
   * @param fromIndex
   * @param splitChars
   * @param expectParts
   * @param maxLength
   */
  splitContent(
    fullText: string,
    fromIndex: number = 0,
    splitChars: string[] = ['ã€‚', 'ï¼Ÿ', 'ï¼', '\n'],
    expectParts: number = 5,
    maxLength: number = 300
  ): {
    text: string
    nextIndex: number
  } {
    // ä¼¼ä¹å‡ºç°äº†ä¸€äº›é—®é¢˜ï¼ŒfromIndex å¤§äºç­‰äº fullText çš„é•¿åº¦ï¼Œç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä¿®å¤ nextIndex ä¸º fullText çš„é•¿åº¦
    if (fromIndex >= fullText.length) {
      return { text: '', nextIndex: fullText.length }
    }
    if (expectParts === 0) {
      return { text: fullText.slice(fromIndex), nextIndex: fullText.length }
    }
    const rest = fullText.slice(fromIndex)
    if (rest.length > maxLength) {
      return this.splitContent(
        fullText,
        fromIndex,
        splitChars,
        expectParts - 1,
        maxLength
      )
    }
    const splitIndexes = rest
      .split('')
      .reduce(
        (acc, char, index) =>
          splitChars.includes(char) ? [...acc, index] : acc,
        [] as number[]
      )
    if (splitIndexes.length >= expectParts) {
      const nextIndex = splitIndexes[expectParts - 1] + fromIndex + 1
      return {
        text: fullText.slice(fromIndex, nextIndex),
        nextIndex,
      }
    }
    return { text: '', nextIndex: fromIndex }
  }

  async reviewConversation(
    base_prompt: string,
    user: string,
    assistant: string
  ) {
    return this.openai.chat.completions
      .create(
        {
          model: this.config.model || 'gpt-4o-mini',
          messages: [
            {
              role: 'system',
              content: this.CENSOR_PROMPT,
            },
            {
              role: 'user',
              content: JSON.stringify({ base_prompt, user, assistant }),
            },
          ],
        },
        {
          timeout: 30 * 1000,
        }
      )
      .then((data) => {
        const text = data.choices?.[0]?.message?.content?.trim()
        console.info('[review]', text, data)
        return text === 'Y'
      })
      .catch((e) => {
        console.error('[review] ERROR', e)
        return true
      })
  }

  async createTTS(
    input: string,
    options?: Partial<OpenAI.Audio.Speech.SpeechCreateParams>
  ) {
    const data = await this.openai.audio.speech.create({
      model: 'tts-1',
      voice: 'nova',
      input,
      response_format: 'mp3',
      speed: 1,
      ...options,
    })
    return data.arrayBuffer()
  }

  static readPromptFile(file: string) {
    try {
      return readFileSync(resolve(__dirname, `./prompts/${file}`), {
        encoding: 'utf-8',
      })
        .toString()
        .trim()
    } catch (e) {
      return ''
    }
  }

  async getChatHistoriesById(
    conversation_id: string,
    limit = 10
  ): Promise<OpenAIConversationLog[]> {
    return (
      ((
        await this.ctx.database.get(
          'openai_chat',
          { conversation_id },
          {
            sort: { time: 'desc' },
            limit: Math.min(25, Math.max(0, limit)),
            fields: ['content', 'role'],
          }
        )
      ).reverse() as OpenAIConversationLog[]) || []
    )
  }

  async summarize(channelId: string) {
    const records = this.getRecords(channelId)
    if (records.length < 10) {
      return <>ğŸ¥€å•Šå“¦â€”â€”ä¿å­˜çš„èŠå¤©è®°å½•å¤ªå°‘äº†ï¼Œéš¾ä»¥è¿›è¡Œæ€»ç»“â€¦â€¦</>
    }

    const recordsText = this.formatRecords(records)

    return this.openai.chat.completions
      .create(
        {
          model: this.config.model || 'gpt-3.5-turbo',
          messages: [
            {
              role: 'system',
              content: this.CHAT_SUMMARY_PROMPT,
            },
            { role: 'user', content: recordsText },
          ],
          max_tokens: this.config.maxTokens ?? 500,
        },
        { timeout: 90 * 1000 }
      )
      .then((data) => {
        this.logger.info('chat-summary', data)
        const text = data.choices?.[0]?.message?.content?.trim()
        if (!text) {
          return (
            <>
              <p>ğŸ’©å™—é€šâ€”â€”è¿›è¡Œæ€»ç»“æ—¶å‡ºç°äº†ä¸€äº›é—®é¢˜ï¼š</p>
              <p>Error è¿”å›ç»“æœä¸ºç©º</p>
            </>
          )
        }
        return (
          <>
            <p>[chat-summary] ä¸‹é¢æ˜¯å¯¹æœ€å{records.length}æ¡èŠå¤©è®°å½•çš„æ€»ç»“ï¼š</p>
            <p></p>
            <p>{text}</p>
          </>
        )
      })
      .catch((e) => {
        return (
          <>
            <p>ğŸ’©å™—é€šâ€”â€”SILIçŒªè„‘è¿‡è½½ï¼</p>
            <p>{'' + e}</p>
          </>
        )
      })
  }

  addRecord(session: Session) {
    const content = session.elements?.join('') || ''
    if (content.includes('[chat-summary]')) {
      return
    }
    const records = this.getRecords(session.channelId)
    records.push({ ...session.toJSON(), content })
    this.#chatRecords[session.channelId] = records.slice(
      records.length - this.config.recordsPerChannel
    )
  }
  getRecords(channelId: string): Session.Payload[] {
    this.#chatRecords[channelId] = this.#chatRecords[channelId] || []
    return this.#chatRecords[channelId]
  }
  formatRecords(records: Session.Payload[]) {
    return JSON.stringify(
      records.map((session) => {
        return {
          user: getUserNickFromSession(session),
          msg: session.content,
        }
      })
    )
  }
}
